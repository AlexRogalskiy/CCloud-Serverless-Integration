import com.fasterxml.jackson.databind.ObjectMapper
import java.nio.file.Paths
import java.nio.file.Files

buildscript {
    repositories {
        mavenCentral()
        maven {
            url = uri("https://packages.confluent.io/maven/")
        }
        maven {
            url = uri("https://plugins.gradle.org/m2/")
        }
        maven {
            url = uri("https://jitpack.io")
        }
    }
}


plugins {
    id 'idea'
    id 'eclipse'
    id "com.google.protobuf" version "0.8.17"
    id "com.github.imflog.kafka-schema-registry-gradle-plugin" version "1.1.1"
    id "com.github.davidmc24.gradle.plugin.avro" version "1.0.0"
}

group 'io.confluent.developer'
version '1.0-SNAPSHOT'

sourceCompatibility = JavaVersion.VERSION_11
targetCompatibility = JavaVersion.VERSION_11

repositories {
    mavenCentral()
    maven {
        url = uri("https://packages.confluent.io/maven/")
    }

    maven {
        url = uri("https://jitpack.io")
    }
}

dependencies {
    implementation platform('software.amazon.awssdk:bom:2.10.73')
    implementation 'software.amazon.awssdk:lambda:2.17.24'
    implementation 'org.apache.kafka:kafka-clients:2.8.0'
    implementation 'com.amazonaws:aws-lambda-java-core:1.2.1'
    implementation 'com.amazonaws:aws-lambda-java-events:3.9.0'
    implementation 'software.amazon.awssdk:secretsmanager:2.17.24'
    implementation 'com.fasterxml.jackson.core:jackson-databind:2.12.4'
    implementation 'com.google.code.gson:gson:2.8.8'
    implementation 'org.apache.logging.log4j:log4j-api:2.15.0'
    implementation 'org.apache.logging.log4j:log4j-core:2.15.0'

    implementation 'com.google.protobuf:protobuf-java:3.17.3'
    implementation 'org.apache.avro:avro:1.10.2'
    implementation('org.apache.kafka:kafka-clients:2.8.0!!')
    implementation('io.confluent:kafka-streams-avro-serde:6.1.1') {
        exclude group: 'org.apache.kafka', module: 'kafka-clients'
        exclude group: 'org.apache.kafka', module: 'kafka-streams'
    }
    implementation('io.confluent:kafka-streams-protobuf-serde:6.1.1') {
        exclude group: 'org.apache.kafka', module: 'kafka-clients'
        exclude group: 'org.apache.kafka', module: 'kafka-streams'
    }
    implementation "io.confluent:kafka-avro-serializer:6.1.1"
    implementation "io.confluent:kafka-protobuf-serializer:6.1.1"
    implementation "io.confluent:kafka-protobuf-provider:6.1.1"


    runtimeOnly 'org.apache.logging.log4j:log4j-slf4j18-impl:2.15.0'
    runtimeOnly 'com.amazonaws:aws-lambda-java-log4j2:1.3.0'
    testImplementation 'org.junit.jupiter:junit-jupiter-api:5.7.2'
    testRuntimeOnly 'org.junit.jupiter:junit-jupiter-engine:5.7.2'
}

protobuf {
    generatedFilesBaseDir = "${project.buildDir}/generated-main-proto-java"

    protoc {
        artifact = 'com.google.protobuf:protoc:3.15.3'
    }
}

/*
  This task looks for a file named 'confluent.properties' in the
  src/main/resources directory and creates a JSON file 'aws-cli/aws-ccloud-creds.json'
  and is used by both the create-secret and update-secret scripts for safely
  storing your CCloud connection credentials
 */

class PropertiesToJsonTask extends DefaultTask {
    @TaskAction
    def propsToJson() {
        Map<String, String> baseDatagenConfigs = ["connector.class":"DatagenSource", "output.data.format":"JSON", "tasks.max":"1"]
        ObjectMapper objectMapper = new ObjectMapper();
        Properties properties = new Properties();
        // We do this to always grab the latest java-service-configs file from the stack-configs directory if previous ones aren't deleted
        File stackConfigs = Files.list(Paths.get("stack-configs"))
                .sorted((path1, path2) -> path2.toFile().lastModified() <=> path1.toFile().lastModified())
                .findFirst().get().toFile()

        try (FileInputStream fis = new FileInputStream(stackConfigs)) {
            properties.load(fis);
        }
        // Taking these out because they aren't sensitive values plus we want the same entries between different deployments
        ["client.dns.lookup", "acks", "basic.auth.credentials.source", "replication.factor"].forEach(k -> properties.remove(k))
        String saslConfig = properties.get("sasl.jaas.config")
        Map<String, String> azureCredentials = ["security.protocol":"SASL_SSL", "sasl.mechanisms":"PLAIN"]
        Map<String, String> azureSchemaRegistryCredentials = ["schema.registry.basic.auth.credentials.source": "USER_INFO"]
        azureCredentials.put("bootstrap.servers", (String)properties.get("bootstrap.servers"))
        azureSchemaRegistryCredentials.put("schema.registry.basic.auth.user.info", (String)properties.get("basic.auth.user.info"))
        azureSchemaRegistryCredentials.put("schema.registry.url", (String)properties.get("schema.registry.url"))
        String azureSRCredsJsonFileName = "azure-schema-registry-creds.json"
        String azureProducerCredJsonFileName = "azure-ccloud-producer-creds.json"
        // We're doing this because the AWS security manager wants a separate username and password entry for configuring the event source
        def matcher = saslConfig =~ /.*username='([^']*)'.*password='([^']*)'.*/
        if (matcher.find()) {
            String username = matcher.group(1)
            String password = matcher.group(2)
            properties.put("username", username)
            properties.put("password", password)
            azureCredentials.put("sasl.username", username)
            azureCredentials.put("sasl.password", password)
            baseDatagenConfigs.put("kafka.api.key", username)
            baseDatagenConfigs.put("kafka.api.secret", password)
        } else {
            throw new RuntimeException("Missing required 'username' and 'password' entries ")
        }

        Map<String, String> stocktradeConfigs = new HashMap<>(baseDatagenConfigs)
        stocktradeConfigs.put("quickstart", "STOCK_TRADES")
        stocktradeConfigs.put("kafka.topic", "stocktrade")
        stocktradeConfigs.put("name","StockTradeDatagen")
        Map<String, String> userConfigs = new HashMap<>(baseDatagenConfigs)
        userConfigs.put("quickstart", "USERS")
        userConfigs.put("kafka.topic", "users")
        userConfigs.put("name", "UsersDatagen")
        Map<String, String> azureAppConfigs = new HashMap<>();
        azureAppConfigs.remove("security.protocol")
        azureAppConfigs.remove("sasl.mechanisms")
        azureAppConfigs.put("ccloud-producer-configs", azureProducerCredJsonFileName)
        azureAppConfigs.put("schema-registry-configs", azureSRCredsJsonFileName)
        azureAppConfigs.put("bootstrap-servers", azureCredentials.get("bootstrap.servers"))
        azureAppConfigs.put("sasl-username", azureCredentials.get("sasl.username"))
        azureAppConfigs.put("sasl-password", azureCredentials.get("sasl.password"))



        File destinationJsonFile = new File("aws-cli/aws-ccloud-creds.json")
        File stocktradeDatagenJsonFile = new File("src/main/resources/stocktrade-datagen.json")
        File userDatagenJsonFile = new File("src/main/resources/user-datagen.json")
        File azureProducerCredsJsonFilePath = new File("azure-cli/${azureProducerCredJsonFileName}")
        File azureSRCredsJsonFilePath = new File("azure-cli/${azureSRCredsJsonFileName}")
        File azureAppSettingsJsonFile = new File("azure-cli/azure-app-settings.json")
        
        objectMapper.writerWithDefaultPrettyPrinter().writeValue(destinationJsonFile, properties)
        objectMapper.writerWithDefaultPrettyPrinter().writeValue(stocktradeDatagenJsonFile, stocktradeConfigs)
        objectMapper.writerWithDefaultPrettyPrinter().writeValue(userDatagenJsonFile, userConfigs)
        objectMapper.writerWithDefaultPrettyPrinter().writeValue(azureProducerCredsJsonFilePath, azureCredentials)
        objectMapper.writerWithDefaultPrettyPrinter().writeValue(azureSRCredsJsonFilePath, azureSchemaRegistryCredentials)
        objectMapper.writerWithDefaultPrettyPrinter().writeValue(azureAppSettingsJsonFile, azureAppConfigs)
    }
}

tasks.register("propsToJson", PropertiesToJsonTask)



task buildZip(type: Zip) {
    from compileJava
    from processResources
    into('lib') {
        from configurations.runtimeClasspath
    }
}
